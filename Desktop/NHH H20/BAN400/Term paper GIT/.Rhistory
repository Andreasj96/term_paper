library(shiny)
install.packages("rtweet")
library(rtweet)
?rtweet
?search_tweets
?search_tweets2
?shinyApp
?search_tweets
library(shiny)
library(rtweet)
?search_tweets
install.packages("twitterR")
search_tweets(
ticker_sybol,
n = 18000, #maximum
type = "recent",
include_rts = FALSE,
geocode = NULL,
max_id = NULL,
parse = TRUE,
token = NULL,
retryonratelimit = FALSE,
verbose = TRUE,
lang = "en"
)
?search_tweets
search_tweets(
"ticker_sybol",
n = 18000, #maximum
type = "recent",
include_rts = FALSE,
geocode = NULL,
max_id = NULL,
parse = TRUE,
token = NULL,
retryonratelimit = FALSE,
verbose = TRUE,
lang = "en"
)
install.packages("tidyquant")
library(tidyquant)
read.csv("us_stock_code.csv")
ticker_symbol <- read.csv("us_stock_code.csv")
View(ticker_symbol)
ticker_symbol <-
read.csv("us_stock_code.csv")%>%
select(code)
library(tidyverse)
library(dplyr)
ticker_symbol <-
read.csv("us_stock_code.csv")%>%
select(code)
View(ticker_symbol)
library(SentimentAnalysis)
runExample("01_hello")
runExample("05_sliders")
runExample("07_widgets")
st2 <- search_tweets2(
c("\"data science\"", "rstats OR python"),
n = 500
)
search_tweets(
"AAPL",
n = 1000, #maximum
type = "recent",
include_rts = FALSE,
geocode = NULL,
max_id = NULL,
parse = TRUE,
token = NULL,
retryonratelimit = FALSE,
verbose = TRUE,
lang = "en"
)
pre_df <- function(df){
df$text %>%
gsub("[[:punct:]]", " ", .) %>%        #remove punctuation
gsub("[[:digit:]]", "", .) %>%         #remove digits
tolower() %>%                          #convert to lower case
removeWords(., stopwords("en")) %>%    #remove standard stopwords
gsub('\\b\\w{1,2}\\b','', .) %>%       #remove words of length 1-2
gsub('\\b\\w{21,}\\b','', .) %>%       #remove words of length 21 or more
gsub("\\s(\\s*)", " ", .) %>%          #remove excess whitespace
trimws()                               #remove first space
}
make_dtm <- function(df){
df$text%>%
DocumentTermMatrix(VCorpus(VectorSource()),
control = list(stemming = TRUE )) #needed?
}
tw_df<-function(df){
search_tweets(
df$searchkey,             #search key
n = 1000,            #number of tweets
type = "recent",
include_rts = FALSE, #exclude retweet
geocode = NULL,
max_id = NULL,
parse = TRUE,
token = NULL,
retryonratelimit = FALSE,
verbose = TRUE,
lang = "en" )%>%
mutate(created_day = date(created),
created_time = time(created))
}
#preprocess function
cleaning_tw_df <- function(df){
df$text %>%
gsub("[[:punct:]]", " ", .) %>%        #remove punctuation
gsub("[[:digit:]]", "", .) %>%         #remove digits
tolower() %>%                          #convert to lower case
removeWords(., stopwords("en")) %>%    #remove standard stopwords
gsub('\\b\\w{1,2}\\b','', .) %>%       #remove words of length 1-2
gsub('\\b\\w{21,}\\b','', .) %>%       #remove words of length 21 or more
gsub("\\s(\\s*)", " ", .) %>%          #remove excess whitespace
trimws()                               #remove first space
}
#dtm funtion
make_dtm <- function(df){
df$text%>%
DocumentTermMatrix(VCorpus(VectorSource()),
control = list(stemming = TRUE )) #needed?
}
#TERM PAPER BAN400
library(shiny)
library(rtweet)
library(tidyquant)
library(tidyverse)
library(dplyr)
library(SentimentAnalysis)
?rm
rm(pre_df)
library(ggplot2)
fluidPage(
# Copy the line below to make a text input box
textInput("text", label = h3("Text input"), value = "Enter text..."),
hr(),
fluidRow(column(3, verbatimTextOutput("value")))
)
?hr(),
?hr()
?fluidRow
?textInput
library(wordcloud)
#output-shortterm sentiment analysis base on twitter
DictionaryGI
#output-shortterm sentiment analysis base on twitter
DictionaryGI[1]
#output-shortterm sentiment analysis base on twitter
str(DictionaryGI)
#output-shortterm sentiment analysis base on twitter
dict_H_negative <- DictionaryGI[1]
dict_H_positive <- DictionaryGI[2]
dict_LM_negative <- DictionaryLM[1]
dict_LM_positive <- DictionaryGI[2]
dict_LM_uncertain <- DictionaryLM[3]
str(DictionaryLM)
#output-shortterm sentiment analysis base on twitter
dict_H_negative <- DictionaryGI[1] %>% as.data.frame()
View(dict_H_negative)
View(dict_H_negative)
dict_H_negative <- DictionaryGI[1] %>% as.data.frame() %>% c("words") -> colnames(.)
dict_H_negative <- DictionaryGI[1] %>% as.data.frame() %>% colnames() <- c("words")
#output-shortterm sentiment analysis base on twitter
dict_H_negative <- DictionaryGI[1] %>% as.data.frame() %>% "words" -> colnames()
DictionaryGI[1] %>% as.data.frame() %>% "words" -> colnames(.)-> dict_H_negative
dict_H_negative <- DictionaryGI[1] %>% as.data.frame()
dict_H_positive <- DictionaryGI[2] %>% as.data.frame()
dict_LM_negative <- DictionaryLM[1] %>% as.data.frame()
dict_LM_positive <- DictionaryGI[2] %>% as.data.frame()
dict_LM_uncertain <- DictionaryLM[3] %>% as.data.frame()
?analyzeSentiment
install.packages("EDGAR")
install.packages("edgar")
library(edgar)
?edgar
install.packages("edgarWebR")
library(edgarWebR)
cik_search(aapl)
cik_search("apple")
cik_search()
cik_search("")
sic_codes
senti.df <- getSentiment(cik.no = '38079', form.type = "10-K",
filing.year = 2020)
??ticker
senti.df <- getSentiment(cik.no = '38079', form.type = "10-K",
filing.year = 2019)
#output-longterm sentiment analysis base on 10
readline("https://datafied.api.edgar-online.com/v2/companies?Appkey=7d405ce3e8ddb45e62da90edcc563c54&primarysymbols=aapl&deleted=false&sortby=primarysymbol%20asc")
readLines("https://datafied.api.edgar-online.com/v2/companies?Appkey=7d405ce3e8ddb45e62da90edcc563c54&primarysymbols=aapl&deleted=false&sortby=primarysymbol%20asc")
#output-longterm sentiment analysis base on 10
ticker <-c("aapl")
web_front <-c("https://datafied.api.edgar-online.com/v2/companies?Appkey=7d405ce3e8ddb45e62da90edcc563c54&primarysymbols=")
web_ticker <-c("aapl")
web_back <- c("&deleted=false&sortby=primarysymbol%20asc")
paste(web_front,web_ticker,web_back)
readLines(paste(web_front,web_ticker,web_back))
getFilings(cik.no = 0000320193, "10-K",
2019, downl.permit = "n")
getFilings(cik.no = 0000320193, "10-K",
2019, quarter = c(1, 2, 3,4), downl.permit = "n")
getFilings(cik.no = 0320193, "10-K",
2019, quarter = c(1, 2, 3,4), downl.permit = "n")
getFilings(cik.no = 0000320193, "10-K",
2020, quarter = c(1, 2, 3,4), downl.permit = "n")
getFilings(cik.no = 0000798019, "10-K",
2020, quarter = c(1, 2, 3,4), downl.permit = "n")
getFilings(cik.no = "0000798019", "10-K",
2020, quarter = c(1, 2, 3,4), downl.permit = "n")
getFilings(cik.no = c(0000798019), "10-K",
2020, quarter = c(1, 2, 3,4), downl.permit = "n")
getFilings(cik.no = c(1000180, 38079), c('10-K','10-Q'),
2006, quarter = c(1, 2, 3), downl.permit = "n")
getMgmtDisc(0000320193,2019)
getMgmtDisc(320193,2019)
getMgmtDisc(789019,2019)
cik_search("aapl")
#TERM PAPER BAN400
library(shiny)
library(rtweet)
library(tidyquant)
library(tidyverse)
library(dplyr)
library(SentimentAnalysis)
library(ggplot2)
library(wordcloud)
library(edgar)
library(edgarWebR)
cik_search("aapl")
cik_search("apple")
is_supported_ticker("PRHSX")
install.packages("riingo")
library(riingo)
is_supported_ticker("PRHSX")
is_supported_ticker("aapl")
is_supported_ticker("AAPL")
supported_tickers()
t<-supported_tickers()
View(t)
ticker_symbol <- supported_tickers()%>%
filter(.$exchange == "AMEX" | "NYSE" | "NASDAQ")
?filter
ticker_symbol <- supported_tickers()%>%
filter(exchange == "AMEX" | "NYSE" | "NASDAQ")
ticker_symbol <- supported_tickers()%>%
filter(exchange == "AMEX" | "NYSE" | "NASDAQ")
supported_tickers()%>%
filter(exchange == "AMEX" | "NYSE" | "NASDAQ")
supported_tickers()%>%
filter(exchange == "AMEX" )
supported_tickers()%>%
filter(exchange == "AMEX | NASDAQ" )
ticker_symbol <- supported_tickers()%>%
filter(exchange == "AMEX" | exchange == "NASDAQ" | exchange == "NYSE" )
View(ticker_symbol)
View(ticker_symbol)
cik_search("AAPL")
?cik_search()
company_href("0000037912")
readLines(paste(web_front,web_ticker,web_back))
readLines(paste(web_front,web_ticker,web_back))%>%
gsub("cik.+(\\d{10}).+", .)
readLines(paste(web_front,web_ticker,web_back))%>%
gsub("cik.+\"(\\d{10})\".+","\\1", .)
readLines(paste(web_front,web_ticker,web_back))%>%
gsub(".*cik.+\"(\\d{10})\".+","\\1", .)
get_cik <- readLines(paste(web_front,web_ticker,web_back))%>%
gsub(".*cik.+\"(\\d{10})\".+","\\1", .)%>%
gsub("0*\"(\\d+)\".*","\\1", . )
get_cik <- readLines(paste(web_front,web_ticker,web_back))%>%
gsub(".*cik.+\"(\\d{10})\".+","\\1", .)%>%
gsub("0*\"(\\d+)\"","\\1", . )
}
get_cik <- readLines(paste(web_front,web_ticker,web_back))%>%
gsub(".*cik.+\"(\\d{10})\".+","\\1", .)%>%
gsub("0*\"(\\d+)\"","\\1", . )
get_cik <- readLines(paste(web_front,web_ticker,web_back))%>%
gsub(".*cik.+\"(\\d{10})\".+","\\1", .)
readLines(paste(web_front,web_ticker,web_back))%>%
gsub(".*cik.+\"(\\d{10})\".+","\\1", .)
readLines(paste(web_front,web_ticker,web_back))%>%
gsub(".*cik.+\"(\\d{10})\".+","\\1", .)%>%
gsub("0*\"(\\d+)\"","\\1", . )
readLines(paste(web_front,web_ticker,web_back))%>%
gsub(".*cik.+\"(\\d{10})\".+","\\1", .)%>%
gsub("0*","", . )
?getMgmtDisc
getMgmtDisc(cik.no = 37912, filling.year = 2020)
getMgmtDisc(cik.no = 37912, filing.year = 2020)
load("C:/Users/windy/Desktop/NHH-BA/R/data science/term_paper/Desktop/NHH H20/BAN400/Term paper GIT/Master Indexes/2020master.Rda")
load("C:/Users/windy/Desktop/NHH-BA/R/data science/term_paper/Desktop/NHH H20/BAN400/Term paper GIT/Master Indexes/2020master.Rda")
View(year.master)
View(year.master)
rm(year.master)
getMgmtDisc(cik.no = 37912, filing.year = 2020)
getMgmtDisc(cik.no = 320193, filing.year = 2020)
getFilings(cik.no = 320193 , form.type = "10-k", filing.year = 2020, downl.permit = n)
readLines(paste(web_front,web_ticker,web_back))%>%
gsub(".*cik.+\"(\\d{10})\".+","\\1", .)%>%
gsub("0*\"(\\d+)\"","\\1", . )
readLines(paste(web_front,web_ticker,web_back))%>%
gsub(".*cik.+\"(\\d{10})\".+","\\1", .)%>%
gsub("^0*\"(\\d+)\"","\\1", . )
readLines(paste(web_front,web_ticker,web_back))%>%
gsub(".*cik.+\"(\\d{10})\".+","\\1", .)%>%
gsub("^0*\"(\\d+)","\\1", . )
readLines(paste(web_front,web_ticker,web_back))%>%
gsub(".*cik.+\"(\\d{10})\".+","\\1", .)%>%
gsub("^0*\"(\\d+)","", . )
readLines(paste(web_front,web_ticker,web_back))%>%
gsub(".*cik.+\"(\\d{10})\".+","\\1", .)%>%
gsub("^0*\"(\\d+)\".*","\\1", . )
readLines(paste(web_front,web_ticker,web_back))%>%
gsub(".*cik.+\"(\\d{10})\".+","\\1", .)%>%
gsub("^0+\"(\\d+)\".*","\\1", . )
readLines(paste(web_front,web_ticker,web_back))%>%
gsub(".*cik.+\"(\\d{10})\".+","\\1", .)%>%
gsub("^0+\"(\\d+)\"","\\1", . )
readLines(paste(web_front,web_ticker,web_back))%>%
gsub(".*cik.+\"(\\d{10})\".+","\\1", .)%>%
gsub("^0+\"(\\d{6})\"","\\1", . )
readLines(paste(web_front,web_ticker,web_back))%>%
gsub(".*cik.+\"(\\d{10})\".+","\\1", .)%>%
gsub("^0+\"(\\d+)\"","\\1", . )
getFilings(cik.no = 0000320193 , form.type = "10-k", filing.year = 2020, downl.permit = n)
getFilings(cik.no = 320193 , form.type = "10-k", filing.year = 2020, downl.permit = n)
getFilings(cik.no = 0000320193 , form.type = "10-k", filing.year = 2020, downl.permit = n)
getFilings(cik.no = 0000320193 , form.type = "10-K", filing.year = 2020, downl.permit = n)
getFilings(cik.no = 320193 , form.type = "10-K", filing.year = 2020, downl.permit = n)
getMgmtDisc(cik.no = 320193, filing.year = 2020)
getFilings(cik.no = 320193 , form.type = "10-K", filing.year = 2020, downl.permit = "n")
getMgmtDisc(cik.no = 320193, filing.year = 2020)
full_text(
type = "10-K",
reverse_order = FALSE,
count = 1,
stemming = TRUE,
cik = "0000320193",
from = "12/31/2019",
to = "12/31/2020"
)
full_text(
"apple"
type = "10-K",
reverse_order = FALSE,
count = 1,
stemming = TRUE,
cik = "0000320193",
from = "12/31/2019",
to = "12/31/2020"
)
full_text(
"apple",
type = "10-K",
reverse_order = FALSE,
count = 1,
stemming = TRUE,
cik = "0000320193",
from = "12/31/2019",
to = "12/31/2020"
)
