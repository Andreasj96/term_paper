library(SentimentAnalysis)
library(ggplot2)
library(wordcloud)
library(edgar)
library(edgarWebR)
library(riingo)
library(anytime)
library(tm)
library(wordcloud)
library(rlist)
library(SnowballC)
library(quantmod)
View(t1)
remove_URL <- function(df){
data <- data.frame(text= character(), stringsAsFactors=FALSE)
for (i in 1:nrow(df)){
extractURL1 <-
df$urls_t.co[i]%>%
unlist()
extractURL2 <-
df$media_t.co[i]%>%
unlist()
combineURL <- c(extractURL1,extractURL2)%>%
unique()
data[i,1] <- removeWords( df$text[i], combineURL)
}
print(data)
}
t3 <- remove_URL(t1)
data <- data.frame(text= character(), stringsAsFactors=FALSE)
for (i in 1:nrow(t1)){
extractURL1 <-
t1$urls_t.co[i]%>%
unlist()
extractURL2 <-
t1$media_t.co[i]%>%
unlist()
combineURL <- c(extractURL1,extractURL2)%>%
unique()
data[i,1] <- removeWords( t1$text[i], combineURL)
}
print(data)
data <- data.frame(text= character(), stringsAsFactors=FALSE)
for (i in 1:nrow(t1)){
extractURL1 <-
t1$urls_t.co[i]%>%
unlist()
extractURL2 <-
t1$media_t.co[i]%>%
unlist()
combineURL <- c(extractURL1,extractURL2)%>%
unique()
data[i,1] <- removeWords( t1$text[i], combineURL)
}
print(data)
extractURL1 <-
t1$urls_t.co[i]%>%
unlist()
extractURL2 <-
t1$media_t.co[i]%>%
unlist()
combineURL <- c(extractURL1,extractURL2)%>%
unique()
removeWords( t1$text[1], combineURL)
is.vector(combineURL)
t1$text[1]
t1$text[1,]
remove_URL <- function(df){
data <- data.frame(text= character(), stringsAsFactors=FALSE)
for (i in 1:nrow(t1)){
extractURL1 <-
df$urls_t.co[i]%>%
unlist()
extractURL2 <-
df$media_t.co[i]%>%
unlist()
combineURL <- c(extractURL1,extractURL2)%>%
unique()
data[i,1] <- removeWords( t1$text[1,], combineURL)
}
print(data)
}
t3 <- remove_URL(t1)
cleaning_tw_df <- function(df){
extract_hashtag <-
DocumentTermMatrix(Corpus(VectorSource(df$hashtags)),
control = list( removePunctuation = T,
stripWhitespace = T,
tolower = T ))%>%
as.matrix()%>%
as.data.frame()%>%
colnames()
#extract hashtags cantained in each tweets
extract_ticker <-
DocumentTermMatrix(Corpus(VectorSource(df$symbols)),
control = list( removePunctuation = T,
stripWhitespace = T,
tolower = T ))%>%
as.matrix()%>%
as.data.frame()%>%
colnames()
#extract ticker symbols cantained in each tweets
remove_URL <- function(df){
data <- data.frame(text= character(), stringsAsFactors=FALSE)
for (i in 1:nrow(df)){
extractURL1 <-
df$urls_t.co[i]%>%
unlist()
extractURL2 <-
df$media_t.co[i]%>%
unlist()
combineURL <- c(extractURL1,extractURL2)%>%
unique()
data[i,1] <- removeWords( df$text[i], combineURL)
}
print(data)
}
text <-
df %>%
remove_URL()
temp <- df%>%
select(-text)%>%
cbind(text)
cleaned_text <-
temp$text.V1%>%
tolower() %>%                          #convert to lower case
gsub("[[:punct:]]", " ", .) %>%        #remove punctuation
gsub("amp"," ", .)%>%                  #remove amp
gsub("https", " ", .)%>%               #remove https
#removeWords(.,extract_hashtag)%>%      #remove hashtags
# removeWords(.,extract_ticker)%>%       #remove tickers symbols
gsub("[[:digit:]]", "", .) %>%         #remove digits
removeWords(., stopwords("en")) %>%    #remove standard stopwords
gsub('\\b\\w{1,2}\\b','', .) %>%       #remove words of length 1-2
gsub("\\s(\\s*)", " ", .) %>%          #remove excess whitespace
trimws()%>%                            #remove first space
as.matrix()%>%
as.data.frame()
#get cleaned tweet text
cleaned_tw <-
df%>%
mutate(text = cleaned_text)%>%
select(user_id, text, source, favorite_count, retweet_count, quote_count, reply_count, followers_count,
favourites_count, created_day, created_time, created_datetime)
#replace text with cleaned text and select valuabe columns
}
t4 <- cleaning_tw_df(t1)
cleaning_tw_df <- function(df){
extract_hashtag <-
DocumentTermMatrix(Corpus(VectorSource(df$hashtags)),
control = list( removePunctuation = T,
stripWhitespace = T,
tolower = T ))%>%
as.matrix()%>%
as.data.frame()%>%
colnames()
#extract hashtags cantained in each tweets
extract_ticker <-
DocumentTermMatrix(Corpus(VectorSource(df$symbols)),
control = list( removePunctuation = T,
stripWhitespace = T,
tolower = T ))%>%
as.matrix()%>%
as.data.frame()%>%
colnames()
#extract ticker symbols cantained in each tweets
remove_URL <- function(df){
data <- data.frame(text= character(), stringsAsFactors=FALSE)
for (i in 1:nrow(df)){
extractURL1 <-
df$urls_t.co[i]%>%
unlist()
extractURL2 <-
df$media_t.co[i]%>%
unlist()
combineURL <- c(extractURL1,extractURL2)%>%
unique()
data[i,1] <- removeWords( df$text[i], combineURL)
}
print(data)
}
cleaned_text <-
df%>%
remove_URL%>%
tolower() %>%                          #convert to lower case
gsub("[[:punct:]]", " ", .) %>%        #remove punctuation
gsub("amp"," ", .)%>%                  #remove amp
gsub("https", " ", .)%>%               #remove https
#removeWords(.,extract_hashtag)%>%      #remove hashtags
# removeWords(.,extract_ticker)%>%       #remove tickers symbols
gsub("[[:digit:]]", "", .) %>%         #remove digits
removeWords(., stopwords("en")) %>%    #remove standard stopwords
gsub('\\b\\w{1,2}\\b','', .) %>%       #remove words of length 1-2
gsub("\\s(\\s*)", " ", .) %>%          #remove excess whitespace
trimws()%>%                            #remove first space
as.matrix()%>%
as.data.frame()
#get cleaned tweet text
cleaned_tw <-
df%>%
mutate(text = cleaned_text)%>%
select(user_id, text, source, favorite_count, retweet_count, quote_count, reply_count, followers_count,
favourites_count, created_day, created_time, created_datetime)
#replace text with cleaned text and select valuabe columns
}
remove_URL <- function(df){
data <- data.frame(text= character(), stringsAsFactors=FALSE)
for (i in 1:nrow(t1)){
extractURL1 <-
df$urls_t.co[i]%>%
unlist()
extractURL2 <-
df$media_t.co[i]%>%
unlist()
combineURL <- c(extractURL1,extractURL2)%>%
unique()
data[i,1] <- removeWords( t1$text[1,], combineURL)
}
print(data)
}
cleaning_tw_df <- function(df){
extract_hashtag <-
DocumentTermMatrix(Corpus(VectorSource(df$hashtags)),
control = list( removePunctuation = T,
stripWhitespace = T,
tolower = T ))%>%
as.matrix()%>%
as.data.frame()%>%
colnames()
#extract hashtags cantained in each tweets
extract_ticker <-
DocumentTermMatrix(Corpus(VectorSource(df$symbols)),
control = list( removePunctuation = T,
stripWhitespace = T,
tolower = T ))%>%
as.matrix()%>%
as.data.frame()%>%
colnames()
#extract ticker symbols cantained in each tweets
remove_URL <- function(df){
data <- data.frame(text= character(), stringsAsFactors=FALSE)
for (i in 1:nrow(t1)){
extractURL1 <-
df$urls_t.co[i]%>%
unlist()
extractURL2 <-
df$media_t.co[i]%>%
unlist()
combineURL <- c(extractURL1,extractURL2)%>%
unique()
data[i,1] <- removeWords( t1$text[1,], combineURL)
}
print(data)
}
cleaned_text <-
df%>%
remove_URL%>%
tolower() %>%                          #convert to lower case
gsub("[[:punct:]]", " ", .) %>%        #remove punctuation
gsub("amp"," ", .)%>%                  #remove amp
gsub("https", " ", .)%>%               #remove https
#removeWords(.,extract_hashtag)%>%      #remove hashtags
# removeWords(.,extract_ticker)%>%       #remove tickers symbols
gsub("[[:digit:]]", "", .) %>%         #remove digits
removeWords(., stopwords("en")) %>%    #remove standard stopwords
gsub('\\b\\w{1,2}\\b','', .) %>%       #remove words of length 1-2
gsub("\\s(\\s*)", " ", .) %>%          #remove excess whitespace
trimws()%>%                            #remove first space
as.matrix()%>%
as.data.frame()
#get cleaned tweet text
cleaned_tw <-
df%>%
mutate(text = cleaned_text)%>%
select(user_id, text, source, favorite_count, retweet_count, quote_count, reply_count, followers_count,
favourites_count, created_day, created_time, created_datetime)
#replace text with cleaned text and select valuabe columns
}
t4 <- cleaning_tw_df(t1)
View(t4)
cleaned_text <-
t1%>%
remove_URL()
cleaned_text <-
t1%>%
remove_URL()%>%
tolower()
?tolower()
cleaned_text <-
t1%>%
remove_URL()%>%
as.vector()%>%
tolower()
cleaned_text <-
t1%>%
remove_URL()%>%
as.vector()
remove_URL <- function(df){
data <- NA
for (i in 1:nrow(t1)){
extractURL1 <-
df$urls_t.co[i]%>%
unlist()
extractURL2 <-
df$media_t.co[i]%>%
unlist()
combineURL <- c(extractURL1,extractURL2)%>%
unique()
data[i] <- removeWords( t1$text[1,], combineURL)
}
print(data)
}
cleaned_text <-
t1%>%
remove_URL()
cleaned_text <-
t1%>%
remove_URL()%>%
tolower()
cleaned_text <-
t1%>%
remove_URL()
remove_URL <- function(df){
data <- NA
for (i in 1:nrow(df)){
extractURL1 <-
df$urls_t.co[i]%>%
unlist()
extractURL2 <-
df$media_t.co[i]%>%
unlist()
combineURL <- c(extractURL1,extractURL2)%>%
unique()
data[i] <- removeWords( t1$text[1,], combineURL)
}
print(data)
}
cleaned_text <-
t1%>%
remove_URL()
remove_URL <- function(df){
data <- data.frame(text= character(), stringsAsFactors=FALSE)
for (i in 1:nrow(df)){
extractURL1 <-
df$urls_t.co[i]%>%
unlist()
extractURL2 <-
df$media_t.co[i]%>%
unlist()
combineURL <- c(extractURL1,extractURL2)%>%
unique()
data[i,1] <- removeWords( t1$text[1,], combineURL)
}
print(data)
}
t3 <- remove_URL(t1)
remove_URL <- function(df){
data <- NA
for (i in 1:nrow(df)){
extractURL1 <-
df$urls_t.co[i]%>%
unlist()
extractURL2 <-
df$media_t.co[i]%>%
unlist()
combineURL <- c(extractURL1,extractURL2)%>%
unique()
data[i] <- removeWords( t1$text[i,], combineURL)
}
print(data)
}
cleaned_text <-
t1%>%
remove_URL()
cleaned_text <-
t1%>%
remove_URL()%>%
tolower() %>%                          #convert to lower case
gsub("[[:punct:]]", " ", .) %>%        #remove punctuation
gsub("amp"," ", .)%>%                  #remove amp
gsub("https", " ", .)%>%               #remove https
#removeWords(.,extract_hashtag)%>%      #remove hashtags
# removeWords(.,extract_ticker)%>%       #remove tickers symbols
gsub("[[:digit:]]", "", .) %>%         #remove digits
removeWords(., stopwords("en")) %>%    #remove standard stopwords
gsub('\\b\\w{1,2}\\b','', .) %>%       #remove words of length 1-2
gsub("\\s(\\s*)", " ", .) %>%          #remove excess whitespace
trimws()%>%                            #remove first space
as.matrix()%>%
as.data.frame()
View(cleaned_text)
cleaned_tw <-
t1%>%
mutate(text = cleaned_text)%>%
select(user_id, text, source, favorite_count, retweet_count, quote_count, reply_count, followers_count,
favourites_count, created_day, created_time, created_datetime)
View(cleaned_tw)
cleaned_text <-
t1%>%
remove_URL()%>%
tolower() %>%                          #convert to lower case
gsub("[[:punct:]]", " ", .) %>%        #remove punctuation
gsub("amp"," ", .)%>%                  #remove amp
gsub("https", " ", .)%>%               #remove https
#removeWords(.,extract_hashtag)%>%      #remove hashtags
# removeWords(.,extract_ticker)%>%       #remove tickers symbols
gsub("[[:digit:]]", "", .) %>%         #remove digits
removeWords(., stopwords("en")) %>%    #remove standard stopwords
gsub('\\b\\w{1,2}\\b','', .) %>%       #remove words of length 1-2
gsub("\\s(\\s*)", " ", .) %>%          #remove excess whitespace
trimws()%>%                            #remove first space
as.matrix()%>%
as.data.frame()
colnames(cleaned_text) <-c( "text")
View(cleaned_text)
cleaned_tw <-
t1%>%
mutate(text = cleaned_text)%>%
select(user_id, text, source, favorite_count, retweet_count, quote_count, reply_count, followers_count,
favourites_count, created_day, created_time, created_datetime)
View(cleaned_text)
View(cleaned_tw)
cleaned_tw <-
t1%>%
select(-text)%>%
mutate(text = cleaned_text)%>%
select(user_id, text, source, favorite_count, retweet_count, quote_count, reply_count, followers_count,
favourites_count, created_day, created_time, created_datetime)
View(cleaned_tw)
cleaned_tw <-
t1%>%
select(-text)
View(cleaned_text)
View(cleaned_tw)
cleaned_tw <-
t1%>%
select(-text)%>%
cbind(cleaned_text)%>%
select(user_id, text, source, favorite_count, retweet_count, quote_count, reply_count, followers_count,
favourites_count, created_day, created_time, created_datetime)
View(cleaned_text)
View(cleaned_text)
View(cleaned_tw)
cleaned_tw <-
t1%>%
select(-text)%>%
cbind(cleaned_text)%>%
select(user_id, text, source, favorite_count, retweet_count, quote_count, reply_count, followers_count,
favourites_count, created_day, created_time, created_datetime)
View(cleaned_tw)
cleaning_tw_df <- function(df){
extract_hashtag <-
DocumentTermMatrix(Corpus(VectorSource(df$hashtags)),
control = list( removePunctuation = T,
stripWhitespace = T,
tolower = T ))%>%
as.matrix()%>%
as.data.frame()%>%
colnames()
#extract hashtags cantained in each tweets
extract_ticker <-
DocumentTermMatrix(Corpus(VectorSource(df$symbols)),
control = list( removePunctuation = T,
stripWhitespace = T,
tolower = T ))%>%
as.matrix()%>%
as.data.frame()%>%
colnames()
#extract ticker symbols cantained in each tweets
remove_URL <- function(df){
data <- NA
for (i in 1:nrow(df)){
extractURL1 <-
df$urls_t.co[i]%>%
unlist()
extractURL2 <-
df$media_t.co[i]%>%
unlist()
combineURL <- c(extractURL1,extractURL2)%>%
unique()
data[i] <- removeWords( t1$text[i,], combineURL)
}
print(data)
}
cleaned_text <-
df%>%
remove_URL()%>%
tolower() %>%                          #convert to lower case
gsub("[[:punct:]]", " ", .) %>%        #remove punctuation
gsub("amp"," ", .)%>%                  #remove amp
gsub("https", " ", .)%>%               #remove https
removeWords(.,extract_hashtag)%>%      #remove hashtags
removeWords(.,extract_ticker)%>%       #remove tickers symbols
gsub("[[:digit:]]", "", .) %>%         #remove digits
removeWords(., stopwords("en")) %>%    #remove standard stopwords
gsub('\\b\\w{1,2}\\b','', .) %>%       #remove words of length 1-2
gsub("\\s(\\s*)", " ", .) %>%          #remove excess whitespace
trimws()%>%                            #remove first space
as.matrix()%>%
as.data.frame()
colnames(cleaned_text) <-c( "text")
#get cleaned tweet text
cleaned_tw <-
df%>%
select(-text)%>%
cbind(cleaned_text)%>%
select(user_id, text, source, favorite_count, retweet_count, quote_count, reply_count, followers_count,
favourites_count, created_day, created_time, created_datetime)
#replace text with cleaned text and select valuabe columns
}
t4 <- cleaning_tw_df(t1)
View(t4)
