to = "12/31/2020"
)
full_text(
"apple"
type = "10-K",
reverse_order = FALSE,
count = 1,
stemming = TRUE,
cik = "0000320193",
from = "12/31/2019",
to = "12/31/2020"
)
full_text(
"apple",
type = "10-K",
reverse_order = FALSE,
count = 1,
stemming = TRUE,
cik = "0000320193",
from = "12/31/2019",
to = "12/31/2020"
)
ticker_symbol <- supported_tickers()%>%
filter(exchange == "AMEX" | exchange == "NASDAQ" | exchange == "NYSE" )
library(shiny)
library(rtweet)
library(tidyquant)
library(tidyverse)
library(dplyr)
library(SentimentAnalysis)
library(ggplot2)
library(wordcloud)
library(edgar)
library(edgarWebR)
ticker_symbol <- supported_tickers()%>%
filter(exchange == "AMEX" | exchange == "NASDAQ" | exchange == "NYSE" )
library(riingo)
ticker_symbol <- supported_tickers()%>%
filter(exchange == "AMEX" | exchange == "NASDAQ" | exchange == "NYSE" )
View(ticker_symbol)
full_text(
"apple",
type = "10-K",
reverse_order = FALSE,
count = 1,
stemming = TRUE,
cik = "0000320193",
from = "12/31/2019",
to = "12/31/2020"
)
get_cik <- function(df){
cik_web_front <-c("https://datafied.api.edgar-online.com/v2/companies?Appkey=7d405ce3e8ddb45e62da90edcc563c54&primarysymbols=")
cik_web_ticker <-c("aapl")  #web_ticker <- tolower(df)
cik_web_back <- c("&deleted=false&sortby=primarysymbol%20asc")
readLines(paste(cik_web_front,cik_web_ticker,cik_web_back))%>%
gsub(".*cik.+\"(\\d{10})\".+","\\1", .)
}
readLines("https://datafied.api.edgar-online.com/v2/companies?primarysymbols=aapl&appkey={7d405ce3e8ddb45e62da90edcc563c54}")
readLines("https://datafied.api.edgar-online.com/v2/companies?primarysymbols=aapl&appkey={7d405ce3e8ddb45e62da90edcc563c54}")
?search_tweets
library(shiny)
library(rtweet)
library(tidyquant)
library(tidyverse)
library(dplyr)
library(SentimentAnalysis)
library(ggplot2)
library(wordcloud)
library(edgar)
library(edgarWebR)
library(riingo)
?search_tweets
token <- create_token(
app = "homework",
consumer_key = "ObtE7neMmt38Y13XycYySboyA",
consumer_secret = "L2xjf9QmC2hyB55GJIAw9Ma2ypekCvMNXKdzpSnvFePncid0vO",
access_token = "1294199419097636864-uJ3M5RfVtYn9EZqVn0t6StWaVnWJxg",
access_token_secret = "7UoNPbQsuN4wRmkLSlElWBuCW6j99FBBVl1OcpkUWHWWe",
)
token <- create_token(
app = "homework",
consumer_key = "ObtE7neMmt38Y13XycYySboyA",
consumer_secret = "L2xjf9QmC2hyB55GJIAw9Ma2ypekCvMNXKdzpSnvFePncid0vO",
access_token = "1294199419097636864-uJ3M5RfVtYn9EZqVn0t6StWaVnWJxg",
access_secret = "7UoNPbQsuN4wRmkLSlElWBuCW6j99FBBVl1OcpkUWHWWe"
)
search_tweets(
"$AAPL",             #search key
n =1000,       #number of tweets
type = "recent",
include_rts = FALSE,          #exclude retweet
geocode = NULL,
max_id = NULL,
parse = TRUE,
token = token,
retryonratelimit = FALSE,
verbose = TRUE,
lang = "en" )
tw_df<-
search_tweets(
"$AAPL",             #search key
n =1000,       #number of tweets
type = "recent",
include_rts = FALSE,          #exclude retweet
geocode = NULL,
max_id = NULL,
parse = TRUE,
token = token,
retryonratelimit = FALSE,
verbose = TRUE,
lang = "en" )
library(shiny)
library(rtweet)
library(tidyquant)
library(tidyverse)
library(dplyr)
library(SentimentAnalysis)
library(ggplot2)
library(wordcloud)
library(edgar)
library(edgarWebR)
library(riingo)
tw_df<-
search_tweets(
"$AAPL",             #search key
n =1000,       #number of tweets
type = "recent",
include_rts = FALSE,          #exclude retweet
geocode = NULL,
max_id = NULL,
parse = TRUE,
token = token,
retryonratelimit = FALSE,
verbose = TRUE,
lang = "en" )
View(tw_df)
View(tw_df)
test_data <- tw_df%>%
mutate(created_day = as.date(created_at),
created_time = as.time(created_at))%>%
select(-created_at)
test_data <- tw_df%>%
mutate(created_day = as.date(.$created_at),
created_time = as.time(.$created_at))%>%
select(-created_at)
test_data <- tw_df%>%
mutate(created_day = as.Date(.$created_at),
created_time = anytime(.$created_at))%>%
select(-created_at)
install.packages("anytime")
library(anytime)
test_data <- tw_df%>%
mutate(created_day = as.Date(.$created_at),
created_time = anytime(.$created_at))%>%
select(-created_at)
View(test_data)
test_data <- tw_df%>%
mutate(created_day = as.Date(.$created_at),
created_time = hour(.$created_at))%>%
select(-created_at)
View(test_data)
test_text <- cleaning_tw_df(test_data)
q
library(tm)
test_text <- cleaning_tw_df(test_data)
test_text <- cleaning_tw_df(test_data)
library(shiny)
library(rtweet)
library(tidyquant)
library(tidyverse)
library(dplyr)
library(SentimentAnalysis)
library(ggplot2)
library(wordcloud)
library(edgar)
library(edgarWebR)
library(riingo)
library(anytime)
library(tm)
test_text <- cleaning_tw_df(test_data)
test_text <- test_data$text%>%
gsub("[[:punct:]]", " ", .) %>%        #remove punctuation
gsub("[[:digit:]]", "", .) %>%         #remove digits
tolower() %>%                          #convert to lower case
removeWords(., stopwords("en")) %>%    #remove standard stopwords
gsub('\\b\\w{1,2}\\b','', .) %>%       #remove words of length 1-2
gsub('\\b\\w{21,}\\b','', .) %>%       #remove words of length 21 or more
gsub("\\s(\\s*)", " ", .) %>%          #remove excess whitespace
trimws()
rm(test_text)
test_text <- cleaning_tw_df(test_data)
test_plot <-  test_data %>%
group_by(created_day, created_time)%>%
summarize(tw_count = n())%>%
ggplot(aes(x=created_day,y = created_time, fill = tw_count))+
geom_point()
test_plot2 <- test_data %>%
group_by(created_day)%>%
summarize(tw_count = n())%>%
ggplot(aes(x=created_day,y = tw_count))+
geom_bar()
test_plot2 <- test_data %>%
group_by(created_day)%>%
summarize(tw_count = n())
test_plot2
test_plot2 <- test_data %>%
group_by(created_day)%>%
summarize(tw_count = n())%>%
ggplot(aes(x=created_day,y = tw_count,fill=tw_count))+
geom_bar()
test_plot2 <- test_data %>%
group_by(created_day)%>%
summarize(tw_count = n())%>%
ggplot(aes(x=created_day,y = tw_count,fill=tw_count))+
geom_col()
test_plot <-  test_data %>%
group_by(created_day, created_time)%>%
summarize(tw_count = n())
test_plot
test_plot%>%
ggplot(aes(x=created_day,y = created_time, fill = tw_count))+
geom_point()
test_plot%>%
ggplot(aes(x=created_day,y = created_time, size = tw_count))+
geom_point()
test_plot%>%
ggplot(aes(x=created_day,y = created_time, size = tw_count))+
geom_point(show.legend = T)+
scale_size_continuous(range=c(2,8))
test_plot%>%
ggplot(aes(x=created_day,y = created_time, size = tw_count))+
geom_point(show.legend = T)+
theme_classic()
test_plot%>%
ggplot(aes(y=created_day,x = created_time, size = tw_count))+
geom_point(show.legend = T)+
theme_classic()
test_plot%>%
ggplot(aes(x=created_day,y= created_time, size = tw_count))+
geom_point(show.legend = T)+
theme_classic()
test_plot%>%
ggplot(aes(x=created_day,y= created_time, size = tw_count))+
geom_point(show.legend = F)+
theme_classic()
test_plot%>%
ggplot(aes(x=created_day,y= created_time, size = tw_count))+
geom_point(alpha=0.5,show.legend = F)+
theme_classic()
test_plot <-  test_data %>%
group_by(created_day, created_time)%>%
summarize(tw_count = n())%>%
ggplot(aes(x=created_day,y= created_time, size = tw_count))+
geom_point(alpha=0.5,show.legend = F)+
theme_classic()
test_plot <-  test_data %>%
group_by(created_day, created_time)%>%
summarize(tw_count = n())%>%
ggplot(aes(x=created_day,y= created_time, size = tw_count))+
geom_point(alpha=0.5,show.legend = F)+
theme_classic()
test_plot <-  test_data %>%
group_by(created_day, created_time)%>%
summarize(tw_count = n())%>%
ungroup()%>%
ggplot(aes(x=created_day,y= created_time, size = tw_count))+
geom_point(alpha=0.5,show.legend = F)+
theme_classic()
test_plot <-  test_data %>%
group_by(created_day, created_time)%>%
summarise(tw_count = n())%>%
ggplot(aes(x=created_day,y= created_time, size = tw_count))+
geom_point(alpha=0.5,show.legend = F)+
theme_classic()
ggplot(data = test_plot,aes(x=created_day,y= created_time, size = tw_count))+
geom_point(alpha=0.5,show.legend = F)+
theme_classic()
test_plot <-  test_data %>%
group_by(created_day, created_time)%>%
summarise(tw_count = n())
ggplot(data = test_plot,aes(x=created_day,y= created_time, size = tw_count))+
geom_point(alpha=0.5,show.legend = F)+
theme_classic()
test_plot2 <- test_data %>%
group_by(created_day)%>%
summarize(tw_count = n())
ggplot(data = test_plot2, aes(x=created_day,y = tw_count,fill=tw_count))+
geom_col()
ggplot(data = test_plot2, aes(x=created_day,y = tw_count,fill=tw_count))+
geom_col(alpha=0.5,show.legend = F)+
theme_classic()
test_sentiment <- test_plot %>%
mutate(harvard_score = analyzeSentiment(text)$SentimentGI)%>%
groupby(created_day)%>%
summarize( daily_harvard_score = mean (harvard_score))
test_sentiment <- test_plot %>%
mutate(harvard_score = analyzeSentiment(text)$SentimentGI)%>%
group_by(created_day)%>%
summarize( daily_harvard_score = mean (harvard_score))
test_plot %>%
mutate(harvard_score = analyzeSentiment(text)$SentimentGI)
test_sentiment <- tw_df %>%
mutate(harvard_score = analyzeSentiment(text)$SentimentGI)%>%
group_by(created_day)%>%
summarize( daily_harvard_score = mean (harvard_score))
test_sentiment <- tw_df %>%
mutate(harvard_score = analyzeSentiment(text)$SentimentGI)
View(test_sentiment)
test_sentiment <- test_data%>%
mutate(harvard_score = analyzeSentiment(text)$SentimentGI)%>%
group_by(created_day)%>%
summarize( daily_harvard_score = mean (harvard_score))
View(test_sentiment)
test_sentiment %>%
ggplot(aes(x= created_day, y= daily_harvard_score))+
geom_point(show.legend = F, col = "red")+
geom_line()
extra_tweets <-function(input.search_key){
search_tweets(
input.search_key,             #search key
n =18000,       #number of tweets
type = "recent",
include_rts = FALSE,          #exclude retweet
geocode = NULL,
max_id = NULL,
parse = TRUE,
token = token,
retryonratelimit = FALSE,
verbose = TRUE,
lang = "en" )%>%
mutate(created_day = as.date(created_at),
created_time = hour(created_at))%>%
select(-created_at)
}
?analyzeSentiment
View(make_dtm)
plot_tweets_dt <- function(df){
df <- df %>%
group_by(created_day, created_time)%>%
summarize(tw_count = n())
plot <- df %>%
ggplot(aes(x=created_day,y = created_time, fill = tw_count))+
geom_point()
print(plot)
}
make_dtm(test_data)
library(shiny)
library(rtweet)
library(tidyquant)
library(tidyverse)
library(dplyr)
library(SentimentAnalysis)
library(ggplot2)
library(wordcloud)
library(edgar)
library(edgarWebR)
library(riingo)
library(anytime)
library(tm)
make_dtm(test_data)
View(test_data)
?DocumentTermMatrix
make_dtm <- function(df){
df$text%>%
?DocumentTermMatrix(VCorpus(VectorSource()),
control = list(bounds = list(global = c(occurance.minimum.docs,
occurance.maximum.docs)) )) }
make_dtm<- function(df){
df$text%>%
DocumentTermMatrix(VCorpus(VectorSource()),
control = list(bounds = list(global = c(occurance.minimum.docs,
occurance.maximum.docs)) )) }
make_dtm(test_data)
make_dtm<- function(df){
df$text%>%
DocumentTermMatrix(VCorpus(VectorSource(.)),
control = list(bounds = list(global = c(occurance.minimum.docs,
occurance.maximum.docs)) )) }
make_dtm(test_data)
make_dtm<- function(df){
df$text%>%
DocumentTermMatrix(Corpus(VectorSource(.)),
control = list(bounds = list(global = c(occurance.minimum.docs,
occurance.maximum.docs)) )) }
make_dtm(test_data)
DocumentTermMatrix(Corpus(VectorSource(test_data$text)))
DocumentTermMatrix(Corpus(VectorSource(test_data$text)))
t <- DocumentTermMatrix(Corpus(VectorSource(test_data$text)))
View(t)
t1 <- as.data.frame(t)
t1 <- as.matrix(t)
t1 <- as.matrix(t)
tttt <- DocumentTermMatrix(Corpus(VectorSource(test_data$text)))
t[["dimnames"]][["Terms"]]fs
tttt <- DocumentTermMatrix(Corpus(VectorSource(test_data$text)))
rm(t)
rm(t1)
t <- DocumentTermMatrix(Corpus(VectorSource(test_data$text)))
library(shiny)
library(rtweet)
library(tidyquant)
library(tidyverse)
library(dplyr)
library(SentimentAnalysis)
library(ggplot2)
library(wordcloud)
library(edgar)
library(edgarWebR)
library(riingo)
library(anytime)
library(tm)
t <- DocumentTermMatrix(Corpus(VectorSource(test_data$text)))
t1 <- as.matrix(t)
library(wordcloud)
x <- t1[1,]
wordcloud(words = names(x),
freq = x,
min.freq = 5)
wordcloud(words = names(x),
freq = x,
min.freq = 5)
x <- t1[1,]
x
View(test_sentiment)
View(test_plot)
t <- DocumentTermMatrix(Corpus(VectorSource(test_text$text)))
t <- DocumentTermMatrix(Corpus(test_text$text))
t <- DocumentTermMatrix(test_text$text)
t <- make_dtm(test_text)
t<-DocumentTermMatrix(Corpus(VectorSource(test_text)))
t1 <- as.matrix(dtm)
t1 <- as.matrix(t)
x <- t1[1,]
wordcloud(words = names(x),
freq = x,
min.freq = 5)
x <- t1[1,]
x
make_dtm<- function(df){
df%>%
DocumentTermMatrix(Corpus(VectorSource(.))) }
test_text[1]
test_text[2]
test_text[3]
test_text[6]
test_text[9]
test_text[40]
test_text[90]
t
View(t)
t1 <- as.matrix(t)
x <- t1[1,]
x
wordcloud(words = names(x),
freq = x,
min.freq = 5)
t1 <- as.data.frame(t)
t1 <- as.data.frame(t)
t1 <- as.matrix(t)
t2 <- as.data.frame((t1)
)
t2 <- as.data.frame(t1)
t2
t1
View(t1)
?colSums()
colSums(t2)
is.vector(colSums(t2))
t3 <- colSums(t2)
t3[1:4]
t4 <- t3[-1]
wordcloud(words = names(t4),
freq = t4,
min.freq = 50)
make_dtm<- function(df){
df%>%
DocumentTermMatrix(Corpus(VectorSource(.)),
control = list(stemming = T,
bounds = list(global = c(1000,
50))
)) }
make_dtm(text_test)
make_dtm(test_text)
make_dtm<- function(df){
df%>%
DocumentTermMatrix(Corpus(VectorSource()),
control = list(stemming = T,
bounds = list(global = c(1000,
50))
)) }
make_dtm(test_text)
make_dtm<- function(df){
DocumentTermMatrix(Corpus(VectorSource(df)),
control = list(stemming = T,
bounds = list(global = c(1000,
50))
)) }
make_dtm(test_text)
q <- make_dtm(test_text)
q <-  DocumentTermMatrix(Corpus(VectorSource(test_text)),
control = list(stemming = T,
bounds = list(global = c(1000,
50))
))
q <-  DocumentTermMatrix(Corpus(VectorSource(test_text)),
control = list(stemming = T,
bounds = list(global = c(1000,
50))))
q1<- as.matrix(q)
q2 <- as.data.frame(q1)
q3 <- colSums(q2)
q3
