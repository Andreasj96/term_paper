DocumentTermMatrix(Corpus(VectorSource(df$symbols)),
control = list( removePunctuation = T,
stripWhitespace = T,
tolower = T ))%>%
as.matrix()%>%
as.data.frame()%>%
colnames()
extract_url1 <-
df$urls_t.co%>%
unlist()%>%
.[is.na(.) == F]
extract_url2 <-
df$media_t.co%>%
unlist()%>%
.[is.na(.) == F]
cleaned_tw <-
df$text %>%
removeWords(.,extract_url1)%>%         #remove url1
removeWords(.,extract_url1)%>%         #remove url2
gsub("U0001.{4}", " ", .)%>%           #doesnot work!!!
gsub("U0001", " ", .)%>%               #doesnot work!!!
gsub("[[:punct:]]", " ", .) %>%        #remove punctuation
tolower() %>%                          #convert to lower case
gsub("amp"," ", .)%>%                  #remove amp
removeWords(.,extract_hashtag)%>%      #remove hashtags
removeWords(.,extract_ticker)%>%       #remove tickers symbols
gsub("[[:digit:]]", "", .) %>%         #remove digits
removeWords(., stopwords("en")) %>%    #remove standard stopwords
gsub('\\b\\w{1,2}\\b','', .) %>%       #remove words of length 1-2
gsub('\\b\\w{21,}\\b','', .) %>%       #remove words of length 21 or more
gsub("\\s(\\s*)", " ", .) %>%          #remove excess whitespace
trimws()                               #remove first space
print(cleaned_tw)
}
t1 <- cleaning_tw_df(tw_df)
make_tweet_wordcloud(t1)
make_tweet_wordcloud<- function(df){
x <-
DocumentTermMatrix(Corpus(VectorSource(df)),
control = list(stemming = T,
bounds = list(global = c(5,500)))) %>%
as.matrix()%>%
as.data.frame()%>%
colSums()
plot_wordcloud <-
wordcloud(words = names(x),
freq = x,
min.freq =20,
max.words = 50,
colors = brewer.pal(5, "Set1"))
print(plot_wordcloud)
}
make_tweet_wordcloud(t1)
cleaning_tw_df <- function(df){
extract_hashtag <-
DocumentTermMatrix(Corpus(VectorSource(df$hashtags)),
control = list( removePunctuation = T,
stripWhitespace = T,
tolower = T ))%>%
as.matrix()%>%
as.data.frame()%>%
colnames()
extract_ticker <-
DocumentTermMatrix(Corpus(VectorSource(df$symbols)),
control = list( removePunctuation = T,
stripWhitespace = T,
tolower = T ))%>%
as.matrix()%>%
as.data.frame()%>%
colnames()
extract_url1 <-
df$urls_t.co%>%
unlist()%>%
.[is.na(.) == F]
extract_url2 <-
df$media_t.co%>%
unlist()%>%
.[is.na(.) == F]
cleaned_tw <-
df$text %>%
removeWords(.,extract_url1)%>%         #remove url1
removeWords(.,extract_url1)%>%         #remove url2
gsub("U0001.{4}", " ", .)%>%           #doesnot work!!!
gsub("U0001", " ", .)%>%               #doesnot work!!!
gsub("[[:punct:]]", " ", .) %>%        #remove punctuation
tolower() %>%                          #convert to lower case
gsub("amp"," ", .)%>%                  #remove amp
gsub("https", " ", .)%>%               #remove https
removeWords(.,extract_hashtag)%>%      #remove hashtags
removeWords(.,extract_ticker)%>%       #remove tickers symbols
gsub("[[:digit:]]", "", .) %>%         #remove digits
removeWords(., stopwords("en")) %>%    #remove standard stopwords
gsub('\\b\\w{1,2}\\b','', .) %>%       #remove words of length 1-2
gsub('\\b\\w{21,}\\b','', .) %>%       #remove words of length 21 or more
gsub("\\s(\\s*)", " ", .) %>%          #remove excess whitespace
trimws()                               #remove first space
print(cleaned_tw)
}
t1 <- cleaning_tw_df(tw_df)
make_tweet_wordcloud(t1)
make_tweet_wordcloud<- function(df){
x <-
DocumentTermMatrix(Corpus(VectorSource(df)),
control = list(stemming = T,
bounds = list(global = c(5,input.number_tweets*2/3)))) %>%
as.matrix()%>%
as.data.frame()%>%
colSums()
plot_wordcloud <-
wordcloud(words = names(x),
freq = x,
min.freq =20,
max.words = 50,
colors = brewer.pal(5, "Set1"))
print(plot_wordcloud)
}
t2 <- daily_harvard_sentiment(t1)
daily_harvard_sentiment <- funtion(df){
daily_score_H <- df %>%
mutate(harvard_score = analyzeSentiment(text)$SentimentGI)%>%
groupby(created_day)%>%
summarize( daily_harvard_score = mean (harvard_score))
if(unique(daily_score_H$created_day) > 1){
daily_score_H%>%
ggplot(aes(x= created_day, y= daily_harvard_score))+
geom_point(show.legend = F, col = "red")+
geom_line()+
scale_x_continuous(name="Date", limits=c(min(created_day)+1, max(created_day)))
geom_smooth(alpha = 0, method = "lm")
}else{
daily_score_H%>%
ggplot(aes(x= created_day, y= daily_harvard_score))+
geom_point(show.legend = F, col = "red")+
geom_line()+
geom_smooth(alpha = 0, method = "lm")
}
}
t2 <- daily_harvard_sentiment(t1)
daily_harvard_sentiment <- funtion(df){
daily_score_H <- df %>%
mutate(harvard_score = analyzeSentiment(text)$SentimentGI)%>%
groupby(created_day)%>%
summarize( daily_harvard_score = mean (harvard_score))
if(unique(daily_score_H$created_day) > 1){
daily_score_H%>%
ggplot(aes(x= created_day, y= daily_harvard_score))+
geom_point(show.legend = F, col = "red")+
geom_line()+
scale_x_continuous(name="Date", limits=c(min(created_day)+1, max(created_day)))
geom_smooth(alpha = 0, method = "lm")
}else{
daily_score_H%>%
ggplot(aes(x= created_day, y= daily_harvard_score))+
geom_point(show.legend = F, col = "red")+
geom_line()+
geom_smooth(alpha = 0, method = "lm")
}
}
daily_harvard_sentiment <- funtion(df){
daily_score_H <- df %>%
mutate(harvard_score = analyzeSentiment(text)$SentimentGI)%>%
groupby(created_day)%>%
summarize( daily_harvard_score = mean (harvard_score))
if(unique(daily_score_H$created_day) > 1){
daily_score_H%>%
ggplot(aes(x= created_day, y= daily_harvard_score))+
geom_point(show.legend = F, col = "red")+
geom_line()+
scale_x_continuous(name="Date", limits=c(min(created_day)+1, max(created_day)))
geom_smooth(alpha = 0, method = "lm")
}else{
daily_score_H%>%
ggplot(aes(x= created_day, y= daily_harvard_score))+
geom_point(show.legend = F, col = "red")+
geom_line()+
geom_smooth(alpha = 0, method = "lm")
}
}
daily_harvard_sentiment <- function(df){
daily_score_H <- df %>%
mutate(harvard_score = analyzeSentiment(text)$SentimentGI)%>%
groupby(created_day)%>%
summarize( daily_harvard_score = mean (harvard_score))
if(unique(daily_score_H$created_day) > 1){
daily_score_H%>%
ggplot(aes(x= created_day, y= daily_harvard_score))+
geom_point(show.legend = F, col = "red")+
geom_line()+
scale_x_continuous(name="Date", limits=c(min(created_day)+1, max(created_day)))
geom_smooth(alpha = 0, method = "lm")
}else{
daily_score_H%>%
ggplot(aes(x= created_day, y= daily_harvard_score))+
geom_point(show.legend = F, col = "red")+
geom_line()+
geom_smooth(alpha = 0, method = "lm")
}
}
t2 <- daily_harvard_sentiment(t1)
daily_score_H <- tw_df %>%
mutate(harvard_score = analyzeSentiment(text)$SentimentGI)%>%
groupby(created_day)%>%
summarize( daily_harvard_score = mean (harvard_score))
daily_score_H <- t1 %>%
mutate(harvard_score = analyzeSentiment(text)$SentimentGI)%>%
groupby(created_day)%>%
summarize( daily_harvard_score = mean (harvard_score))
t1 <- cleaning_tw_df(tw_df)
is.vector(t1)
t1 <- cleaning_tw_df(tw_df)%>%
as.data.frame()
t1 <- cleaning_tw_df(tw_df)%>%
as.matrix()%>%
as.data.frame()
t1 <- cleaning_tw_df(tw_df)%>%
as.matrix()%>%
as.data.frame()
t1
str(t1)
View(tw_df)
t1 <- cleaning_tw_df(test_data)%>%
as.matrix()%>%
as.data.frame()
t1
t1 <- cleaning_tw_df(test_data)
test_data%>%
mutate(text = t1)
colnames(test_data)
test_data%>%
mutate(text = t1)%>%
select(user_id, text, source, favorite_count, retweet_count, quote_count, reply_count, url, followers_count,
favourites_count, created_day, created_time)
test_data%>%
mutate(text = t1)%>%
select(user_id, text, source, favorite_count, retweet_count, quote_count, reply_count, followers_count,
favourites_count, created_day, created_time)
cleaning_tw_df <- function(df){
extract_hashtag <-
DocumentTermMatrix(Corpus(VectorSource(df$hashtags)),
control = list( removePunctuation = T,
stripWhitespace = T,
tolower = T ))%>%
as.matrix()%>%
as.data.frame()%>%
colnames()
#extract hashtags cantained in each tweets
extract_ticker <-
DocumentTermMatrix(Corpus(VectorSource(df$symbols)),
control = list( removePunctuation = T,
stripWhitespace = T,
tolower = T ))%>%
as.matrix()%>%
as.data.frame()%>%
colnames()
#extract ticker symbols cantained in each tweets
extract_url1 <-
df$urls_t.co%>%
unlist()%>%
.[is.na(.) == F]
#extract url cantained in each tweets
extract_url2 <-
df$media_t.co%>%
unlist()%>%
.[is.na(.) == F]
#extract another url cantained in each tweets
cleaned_text <-
df$text %>%
removeWords(.,extract_url1)%>%         #remove url1
removeWords(.,extract_url1)%>%         #remove url2
gsub("U0001.{4}", " ", .)%>%           #doesnot work!!!
gsub("U0001", " ", .)%>%               #doesnot work!!!
gsub("[[:punct:]]", " ", .) %>%        #remove punctuation
tolower() %>%                          #convert to lower case
gsub("amp"," ", .)%>%                  #remove amp
gsub("https", " ", .)%>%               #remove https
removeWords(.,extract_hashtag)%>%      #remove hashtags
removeWords(.,extract_ticker)%>%       #remove tickers symbols
gsub("[[:digit:]]", "", .) %>%         #remove digits
removeWords(., stopwords("en")) %>%    #remove standard stopwords
gsub('\\b\\w{1,2}\\b','', .) %>%       #remove words of length 1-2
gsub("\\s(\\s*)", " ", .) %>%          #remove excess whitespace
trimws()                               #remove first space
#get cleaned tweet text
cleaned_tw <-
df%>%
mutate(text = cleaned_text)%>%
select(user_id, text, source, favorite_count, retweet_count, quote_count, reply_count, followers_count,
favourites_count, created_day, created_time)
#replace text with cleaned text and select valuabe columns
}
re(tw_df)
rm(tw_df)
t1 <- cleaning_tw_df(test_data)
t1
make_tweet_wordcloud<- function(df){
x <-
DocumentTermMatrix(Corpus(VectorSource(df$text)),
control = list(stemming = T,
bounds = list(global = c(5,input.number_tweets*2/3)))) %>%
as.matrix()%>%
as.data.frame()%>%
colSums()
plot_wordcloud <-
wordcloud(words = names(x),
freq = x,
min.freq =20,
max.words = 50,
colors = brewer.pal(5, "Set1"))
print(plot_wordcloud)
}
make_tweet_wordcloud(t1)
make_tweet_wordcloud<- function(df){
x <-
DocumentTermMatrix(Corpus(VectorSource(df$text)),
control = list(stemming = T,
bounds = list(global = c(5,500)))) %>%
as.matrix()%>%
as.data.frame()%>%
colSums()
plot_wordcloud <-
wordcloud(words = names(x),
freq = x,
min.freq =20,
max.words = 50,
colors = brewer.pal(5, "Set1"))
print(plot_wordcloud)
}
make_tweet_wordcloud(t1)
make_tweet_wordcloud<- function(df){
x <-
DocumentTermMatrix(Corpus(VectorSource(df$text)),
control = list(stemming = T,
bounds = list(global = c(5,500)))) %>%   #500 = input.number_tweets
as.matrix()%>%
as.data.frame()%>%
colSums()
plot_wordcloud <-
wordcloud(words = names(x),
freq = x,
min.freq =20,
max.words = 50,
colors = brewer.pal(5, "Set1"))
print(plot_wordcloud)
}
daily_harvard_sentiment <- function(df){
daily_score_H <- df %>%
mutate(harvard_score = analyzeSentiment(text)$SentimentGI)%>%
groupby(created_day)%>%
summarize( daily_harvard_score = mean (harvard_score))
if(unique(daily_score_H$created_day) > 1){
daily_score_H%>%
ggplot(aes(x= created_day, y= daily_harvard_score))+
geom_point(show.legend = F, col = "red")+
geom_line()+
scale_x_continuous(name="Date", limits=c(min(created_day)+1, max(created_day)))
geom_smooth(alpha = 0, method = "lm")
}else{
daily_score_H%>%
ggplot(aes(x= created_day, y= daily_harvard_score))+
geom_point(show.legend = F, col = "red")+
geom_line()+
geom_smooth(alpha = 0, method = "lm")
}
}
daily_harvard_sentiment(t1)
daily_harvard_sentiment <- function(df){
daily_score_H <- df %>%
mutate(harvard_score = analyzeSentiment(text)$SentimentGI)%>%
group_by(created_day)%>%
summarize( daily_harvard_score = mean (harvard_score))
if(unique(daily_score_H$created_day) > 1){
daily_score_H%>%
ggplot(aes(x= created_day, y= daily_harvard_score))+
geom_point(show.legend = F, col = "red")+
geom_line()+
scale_x_continuous(name="Date", limits=c(min(created_day)+1, max(created_day)))
geom_smooth(alpha = 0, method = "lm")
}else{
daily_score_H%>%
ggplot(aes(x= created_day, y= daily_harvard_score))+
geom_point(show.legend = F, col = "red")+
geom_line()+
geom_smooth(alpha = 0, method = "lm")
}
}
daily_harvard_sentiment(t1)
daily_score_H <- t1 %>%
mutate(harvard_score = analyzeSentiment(text)$SentimentGI)
daily_score_H
daily_score_H <- t1 %>%
mutate(harvard_score = analyzeSentiment(text)$SentimentGI)%>%
group_by(created_day)%>%
summarize( daily_harvard_score = mean (harvard_score))
daily_score_H
daily_score_H <- t1 %>%
mutate(harvard_score = analyzeSentiment(text)$SentimentGI
)
summary(daily_score_H)
head(daily_score_H)
daily_score_H$harvard_score
mean(daily_score_H$harvard_score)
?mean
mean(daily_score_H$harvard_score, na.rm = T)
daily_score_H <- t1 %>%
mutate(harvard_score = analyzeSentiment(text)$SentimentGI)%>%
group_by(created_day)%>%
summarize( daily_harvard_score = mean (harvard_score, na.rm = T))
daily_score_H
daily_harvard_sentiment <- function(df){
daily_score_H <- df %>%
mutate(harvard_score = analyzeSentiment(text)$SentimentGI)%>%
group_by(created_day)%>%
summarize( daily_harvard_score = mean (harvard_score, na.rm = T))
if(unique(daily_score_H$created_day) > 1){
daily_score_H%>%
ggplot(aes(x= created_day, y= daily_harvard_score))+
geom_point(show.legend = F, col = "red")+
geom_line()+
scale_x_continuous(name="Date", limits=c(min(created_day)+1, max(created_day)))
geom_smooth(alpha = 0, method = "lm")
}else{
daily_score_H%>%
ggplot(aes(x= created_day, y= daily_harvard_score))+
geom_point(show.legend = F, col = "red")+
geom_line()+
geom_smooth(alpha = 0, method = "lm")
}
}
daily_harvard_sentiment(t1)
daily_score_H <- t1 %>%
mutate(harvard_score = analyzeSentiment(text)$SentimentGI)%>%
group_by(created_day)%>%
summarize( daily_harvard_score = mean (harvard_score, na.rm = T))
daily_score_H <- t1 %>%
mutate(harvard_score = analyzeSentiment(text)$SentimentGI)%>%
group_by(created_day)%>%
summarize( daily_harvard_score = mean (harvard_score, na.rm = T))
unique(daily_score_H$created_day
)
length(unique(daily_score_H$created_day))
if(length(unique(daily_score_H$created_day)) > 1){
daily_score_H%>%
ggplot(aes(x= created_day, y= daily_harvard_score))+
geom_point(show.legend = F, col = "red")+
geom_line()+
scale_x_continuous(name="Date", limits=c(min(created_day)+1, max(created_day)))
geom_smooth(alpha = 0, method = "lm")
}else{
daily_score_H%>%
ggplot(aes(x= created_day, y= daily_harvard_score))+
geom_point(show.legend = F, col = "red")+
geom_line()+
geom_smooth(alpha = 0, method = "lm")
}
min(created_day)
if(length(unique(daily_score_H$created_day)) > 2){
daily_score_H%>%
ggplot(aes(x= created_day, y= daily_harvard_score))+
geom_point(show.legend = F, col = "red")+
geom_line()+
scale_x_continuous(name="Date", limits=c(min(.$created_day)+1, max(.$created_day)))
geom_smooth(alpha = 0, method = "lm")
}else{
daily_score_H%>%
ggplot(aes(x= created_day, y= daily_harvard_score))+
geom_point(show.legend = F, col = "red")+
geom_line()+
geom_smooth(alpha = 0, method = "lm")
}
if(length(unique(daily_score_H$created_day)) > 2){
daily_score_H%>%
ggplot(aes(x= created_day, y= daily_harvard_score))+
geom_point(show.legend = F, col = "red")+
geom_line()+
scale_x_continuous(name="Date", limits=c(min(.$created_day)+1, max(.$created_day)))+
geom_smooth(alpha = 0, method = "lm")
}else{if(length(unique(daily_score_H$created_day)) > 1)
{daily_score_H%>%
ggplot(aes(x= created_day, y= daily_harvard_score))+
geom_point(show.legend = F, col = "red")+
geom_line()+
scale_x_continuous(name="Date", limits= max(.$created_day))
}else{
daily_score_H%>%
ggplot(aes(x= created_day, y= daily_harvard_score))+
geom_point(show.legend = F, col = "red")+
geom_line()+
geom_smooth(alpha = 0, method = "lm")
}
}
daily_score_H%>%
ggplot(aes(x= created_day, y= daily_harvard_score))+
geom_point(show.legend = F, col = "red")+
geom_line()+
scale_x_continuous(name="Date", limits=c(min(.$created_day)+1, max(.$created_day)))+
geom_smooth(alpha = 0, method = "lm")
daily_score_H <- t1 %>%
mutate(harvard_score = analyzeSentiment(text)$SentimentGI)%>%
group_by(created_day)%>%
summarize( daily_harvard_score = mean (harvard_score, na.rm = T))
daily_score_H%>%
ggplot(aes(x= created_day, y= daily_harvard_score))+
geom_point(show.legend = F, col = "red")+
geom_line()+
scale_x_continuous(name="Date", limits=c(min(.$created_day)+1, max(.$created_day)))+
geom_smooth(alpha = 0, method = "lm")
daily_score_H%>%
ggplot(aes(x= created_day, y= daily_harvard_score))+
geom_point(show.legend = F, col = "red")+
geom_line()+
scale_x_continuous(name="Date", limits=c(min(created_day)+1, max(created_day)))+
geom_smooth(alpha = 0, method = "lm")
